{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cdd400d",
   "metadata": {},
   "source": [
    "# Complete OCR Pipeline\n",
    "\n",
    "This notebook demonstrates the complete end-to-end OCR pipeline using CRAFT for text detection and CRNN for text recognition.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Image Preprocessing** - Convert to grayscale, blur, and threshold\n",
    "2. **Text Detection** - CRAFT detects text regions\n",
    "3. **Text Cropping** - Extract detected regions\n",
    "4. **Text Recognition** - CRNN recognizes characters\n",
    "5. **Result Visualization** - Display and save results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eb4574",
   "metadata": {},
   "source": [
    "# Complete OCR Pipeline\n",
    "\n",
    "This notebook demonstrates the complete end-to-end OCR pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd14be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import easyocr\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3467421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_json(results, filename=\"ocr_results.json\"):\n",
    "    \"\"\"Save OCR results to JSON file\"\"\"\n",
    "    output_dir = Path('processed_data')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    output_path = output_dir / filename\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Results saved to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "def display_results_table(results):\n",
    "    \"\"\"Display detection results in table format\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OCR DETECTION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nüìù FULL TEXT:\\n{results['full_text']}\\n\")\n",
    "    print(f\"\\nüîç DETAILED DETECTIONS ({len(results['detections'])} regions):\\n\")\n",
    "    \n",
    "    print(f\"{'No.':<5} {'Text':<30} {'Confidence':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, det in enumerate(results['detections'], 1):\n",
    "        text = det['text'][:25] + \"...\" if len(det['text']) > 25 else det['text']\n",
    "        conf = f\"{det['confidence']:.2%}\"\n",
    "        print(f\"{i:<5} {text:<30} {conf:<15}\")\n",
    "    \n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Results functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5beadfd",
   "metadata": {},
   "source": [
    "## 5. Results Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a19fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for sample images in processed_data/raw\n",
    "raw_dir = Path('processed_data/raw')\n",
    "image_files = list(raw_dir.glob('*.jpg')) + list(raw_dir.glob('*.png')) + list(raw_dir.glob('*.jpeg'))\n",
    "\n",
    "if image_files:\n",
    "    print(f\"Found {len(image_files)} image(s) in {raw_dir}\")\n",
    "    print(\"Sample images:\", [f.name for f in image_files[:3]])\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  No images found in {raw_dir}\")\n",
    "    print(\"üì§ Please add images to 'processed_data/raw/' to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f03bd",
   "metadata": {},
   "source": [
    "## 4. Process Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31be189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_ocr(image_path, reader):\n",
    "    \"\"\"\n",
    "    Extract text from image using EasyOCR (CRAFT + CRNN)\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        reader: EasyOCR reader object\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains full_text, detections with confidence scores\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not read image {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Processing: {image_path}\")\n",
    "    results = reader.readtext(image)\n",
    "    \n",
    "    extracted_data = {\n",
    "        'image_path': image_path,\n",
    "        'full_text': '',\n",
    "        'detections': [],\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    full_text_parts = []\n",
    "    \n",
    "    for detection in results:\n",
    "        bbox, text, confidence = detection\n",
    "        bbox = np.array(bbox, dtype=np.int32)\n",
    "        \n",
    "        extracted_data['detections'].append({\n",
    "            'text': text,\n",
    "            'confidence': float(confidence),\n",
    "            'bbox': bbox.tolist()\n",
    "        })\n",
    "        \n",
    "        full_text_parts.append(text)\n",
    "    \n",
    "    extracted_data['full_text'] = ' '.join(full_text_parts)\n",
    "    \n",
    "    print(f\"  üìù Detected {len(results)} text regions\")\n",
    "    print(f\"  üìä Average confidence: {np.mean([d['confidence'] for d in extracted_data['detections']]):.2%}\")\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "def draw_detections(image_path, results):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes and text labels on image\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        results: Results from extract_text_with_ocr\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: Image with drawn bounding boxes\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image_with_boxes = image.copy()\n",
    "    \n",
    "    for detection in results['detections']:\n",
    "        bbox = np.array(detection['bbox'], dtype=np.int32)\n",
    "        text = detection['text']\n",
    "        confidence = detection['confidence']\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.polylines(image_with_boxes, [bbox], True, (0, 255, 0), 2)\n",
    "        \n",
    "        # Put text label\n",
    "        label = f\"{text} ({confidence:.2f})\"\n",
    "        cv2.putText(image_with_boxes, label, tuple(bbox[0]), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    return image_with_boxes\n",
    "\n",
    "def visualize_results(image_path, results):\n",
    "    \"\"\"Display original image with detections\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image_with_boxes = draw_detections(image_path, results)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title(f\"Detections ({len(results['detections'])} regions)\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ OCR functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b88b4e",
   "metadata": {},
   "source": [
    "## 3. Text Detection and Recognition Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c351f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading EasyOCR reader (CRAFT + CRNN)...\")\n",
    "reader = easyocr.Reader(['en'])\n",
    "print(\"‚úÖ OCR Reader loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf9408",
   "metadata": {},
   "source": [
    "## 2. Initialize EasyOCR (CRAFT + CRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d4b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Preprocess image: convert to grayscale, apply blur, and threshold\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (original, gray, blurred, thresh)\n",
    "    \"\"\"\n",
    "    # Read image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not read image {image_path}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to remove noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Apply thresholding to enhance text\n",
    "    _, thresh = cv2.threshold(blurred, 150, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    return image, gray, blurred, thresh\n",
    "\n",
    "def display_preprocessing_steps(image_path):\n",
    "    \"\"\"Display preprocessing steps\"\"\"\n",
    "    image, gray, blurred, thresh = preprocess_image(image_path)\n",
    "    \n",
    "    if image is None:\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    axes[0, 0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    axes[0, 0].set_title(\"Original Image\")\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(gray, cmap='gray')\n",
    "    axes[0, 1].set_title(\"Grayscale\")\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[1, 0].imshow(blurred, cmap='gray')\n",
    "    axes[1, 0].set_title(\"Gaussian Blur\")\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(thresh, cmap='gray')\n",
    "    axes[1, 1].set_title(\"Thresholded\")\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Preprocessing functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c00ef0",
   "metadata": {},
   "source": [
    "## 1. Image Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77660ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done!\n"
     ]
    }
   ],
   "source": [
    "raw_dir = \"processed_data/raw\"\n",
    "train_dir = \"processed_data/train\"\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "for img_name in os.listdir(raw_dir):\n",
    "    img_path = os.path.join(raw_dir, img_name)\n",
    "    processed = preprocess_image(img_path)\n",
    "    cv2.imwrite(os.path.join(train_dir, img_name), processed)\n",
    "\n",
    "print(\"Preprocessing done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913605be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text_regions(image):\n",
    "    contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    boxes = []\n",
    "    for cnt in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        if w > 30 and h > 10:   # filter noise\n",
    "            boxes.append((x,y,w,h))\n",
    "    return boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3872a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping done!\n"
     ]
    }
   ],
   "source": [
    "crop_dir = \"processed_data/crops\"\n",
    "os.makedirs(crop_dir, exist_ok=True)\n",
    "\n",
    "for img_name in os.listdir(train_dir):\n",
    "    img_path = os.path.join(train_dir, img_name)\n",
    "    img = cv2.imread(img_path, 0)\n",
    "    boxes = detect_text_regions(img)\n",
    "\n",
    "    for i, (x,y,w,h) in enumerate(boxes):\n",
    "        crop = img[y:y+h, x:x+w]\n",
    "        crop_name = f\"{img_name.split('.')[0]}_{i}.jpg\"\n",
    "        cv2.imwrite(os.path.join(crop_dir, crop_name), crop)\n",
    "\n",
    "print(\"Cropping done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ace6b0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.json created!\n"
     ]
    }
   ],
   "source": [
    "labels = {}\n",
    "\n",
    "for img in os.listdir(crop_dir):\n",
    "    labels[img] = \"TEXT\"   # placeholder label for now\n",
    "\n",
    "with open(\"processed_data/labels.json\", \"w\") as f:\n",
    "    json.dump(labels, f, indent=4)\n",
    "\n",
    "print(\"labels.json created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6953760f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytesseract\n",
      "  Using cached pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\krish\\onedrive\\documents\\ocr_pipline\\ocr_env\\lib\\site-packages (12.1.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\krish\\onedrive\\documents\\ocr_pipline\\ocr_env\\lib\\site-packages (from pytesseract) (25.0)\n",
      "Using cached pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytesseract pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_dir = \"processed_data/crops\"\n",
    "\n",
    "for img_name in os.listdir(crop_dir):\n",
    "    img_path = os.path.join(crop_dir, img_name)\n",
    "    img = Image.open(img_path)\n",
    "    text = pytesseract.image_to_string(img)\n",
    "    print(f\"{img_name} ‚Üí {text.strip()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
