{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf53bc5",
   "metadata": {},
   "source": [
    "# OCR Data Processor\n",
    "\n",
    "This notebook handles data preprocessing, organization, and preparation for the OCR pipeline.\n",
    "\n",
    "## Tasks:\n",
    "1. **Organize raw dataset** - Sort images by train/val/test\n",
    "2. **Validate images** - Check format and integrity\n",
    "3. **Generate labels** - Create annotation files\n",
    "4. **Crop text regions** - Save detected text areas\n",
    "5. **Build metadata** - Create index files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56430370",
   "metadata": {},
   "source": [
    "# OCR Data Processor\n",
    "\n",
    "This notebook handles data processing for the OCR pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7949ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcb13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_report(base_dir):\n",
    "    \"\"\"Generate dataset statistics report\"\"\"\n",
    "    base_dir = Path(base_dir)\n",
    "    \n",
    "    report = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'directories': {}\n",
    "    }\n",
    "    \n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        split_dir = base_dir / split_name\n",
    "        if split_dir.exists():\n",
    "            image_files = list(split_dir.glob('*.jpg')) + list(split_dir.glob('*.png')) + list(split_dir.glob('*.jpeg'))\n",
    "            \n",
    "            total_size = sum(f.stat().st_size for f in image_files) / (1024*1024)  # MB\n",
    "            \n",
    "            report['directories'][split_name] = {\n",
    "                'count': len(image_files),\n",
    "                'total_size_mb': round(total_size, 2)\n",
    "            }\n",
    "    \n",
    "    # Total crops\n",
    "    if (base_dir / 'crops').exists():\n",
    "        crop_files = list((base_dir / 'crops').glob('*.png'))\n",
    "        report['crops'] = {\n",
    "            'count': len(crop_files),\n",
    "            'total_size_mb': round(sum(f.stat().st_size for f in crop_files) / (1024*1024), 2)\n",
    "        }\n",
    "    \n",
    "    return report\n",
    "\n",
    "def print_dataset_report():\n",
    "    \"\"\"Print formatted dataset report\"\"\"\n",
    "    report = generate_dataset_report(base_dir)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä DATASET REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for split_name, stats in report.get('directories', {}).items():\n",
    "        print(f\"\\n{split_name.upper()}:\")\n",
    "        print(f\"  Images: {stats['count']}\")\n",
    "        print(f\"  Size: {stats['total_size_mb']} MB\")\n",
    "    \n",
    "    if 'crops' in report:\n",
    "        print(f\"\\nCROPS:\")\n",
    "        print(f\"  Total: {report['crops']['count']}\")\n",
    "        print(f\"  Size: {report['crops']['total_size_mb']} MB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Report functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646b373e",
   "metadata": {},
   "source": [
    "## 5. Generate Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_crops_and_labels(image_path, reader, crop_save_dir=crops_dir):\n",
    "    \"\"\"\n",
    "    Generate cropped text regions and labels from an image\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image file\n",
    "        reader: EasyOCR reader object\n",
    "        crop_save_dir: Directory to save crops\n",
    "        \n",
    "    Returns:\n",
    "        dict: Crop information\n",
    "    \"\"\"\n",
    "    image = cv2.imread(str(image_path))\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    results = reader.readtext(image)\n",
    "    \n",
    "    crops_info = {\n",
    "        'image_name': image_path.name,\n",
    "        'image_path': str(image_path),\n",
    "        'crops': []\n",
    "    }\n",
    "    \n",
    "    for idx, detection in enumerate(results):\n",
    "        bbox, text, confidence = detection\n",
    "        bbox = np.array(bbox, dtype=np.int32)\n",
    "        \n",
    "        # Get bounding rectangle\n",
    "        x_min = min([point[0] for point in bbox])\n",
    "        x_max = max([point[0] for point in bbox])\n",
    "        y_min = min([point[1] for point in bbox])\n",
    "        y_max = max([point[1] for point in bbox])\n",
    "        \n",
    "        # Crop the region with some padding\n",
    "        padding = 5\n",
    "        x_min = max(0, x_min - padding)\n",
    "        y_min = max(0, y_min - padding)\n",
    "        x_max = min(image.shape[1], x_max + padding)\n",
    "        y_max = min(image.shape[0], y_max + padding)\n",
    "        \n",
    "        crop = image[y_min:y_max, x_min:x_max]\n",
    "        \n",
    "        if crop.size > 0:\n",
    "            # Save crop\n",
    "            crop_filename = f\"{image_path.stem}_crop_{idx:03d}.png\"\n",
    "            crop_path = crop_save_dir / crop_filename\n",
    "            cv2.imwrite(str(crop_path), crop)\n",
    "            \n",
    "            crops_info['crops'].append({\n",
    "                'crop_id': idx,\n",
    "                'text': text,\n",
    "                'confidence': float(confidence),\n",
    "                'bbox': [int(x_min), int(y_min), int(x_max), int(y_max)],\n",
    "                'crop_file': crop_filename\n",
    "            })\n",
    "    \n",
    "    return crops_info\n",
    "\n",
    "def process_all_images(directory, reader):\n",
    "    \"\"\"Process all images to generate crops and labels\"\"\"\n",
    "    directory = Path(directory)\n",
    "    all_labels = {'images': []}\n",
    "    \n",
    "    image_files = list(directory.glob('*.jpg')) + list(directory.glob('*.png')) + list(directory.glob('*.jpeg'))\n",
    "    \n",
    "    print(f\"üì∏ Processing {len(image_files)} images...\")\n",
    "    \n",
    "    for i, img_path in enumerate(image_files, 1):\n",
    "        print(f\"  [{i}/{len(image_files)}] {img_path.name}...\", end='')\n",
    "        \n",
    "        crops_info = generate_crops_and_labels(img_path, reader)\n",
    "        if crops_info:\n",
    "            all_labels['images'].append(crops_info)\n",
    "            print(f\" ‚úÖ ({len(crops_info['crops'])} crops)\")\n",
    "        else:\n",
    "            print(\" ‚ùå (failed)\")\n",
    "    \n",
    "    # Save labels\n",
    "    labels_file.parent.mkdir(exist_ok=True)\n",
    "    with open(labels_file, 'w') as f:\n",
    "        json.dump(all_labels, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Labels saved to: {labels_file}\")\n",
    "    return all_labels\n",
    "\n",
    "print(\"‚úÖ Crop and label functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d5708",
   "metadata": {},
   "source": [
    "## 4. Crop and Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529c5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(image_paths, train_ratio=0.7, val_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Split images into train/val/test sets\n",
    "    \n",
    "    Args:\n",
    "        image_paths: List of image file paths\n",
    "        train_ratio: Ratio for training set (default 0.7)\n",
    "        val_ratio: Ratio for validation set (default 0.2)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains train, val, test image lists\n",
    "    \"\"\"\n",
    "    n_images = len(image_paths)\n",
    "    n_train = int(n_images * train_ratio)\n",
    "    n_val = int(n_images * val_ratio)\n",
    "    \n",
    "    np.random.shuffle(image_paths)\n",
    "    \n",
    "    train_images = image_paths[:n_train]\n",
    "    val_images = image_paths[n_train:n_train+n_val]\n",
    "    test_images = image_paths[n_train+n_val:]\n",
    "    \n",
    "    return {\n",
    "        'train': train_images,\n",
    "        'val': val_images,\n",
    "        'test': test_images\n",
    "    }\n",
    "\n",
    "def organize_images(splits):\n",
    "    \"\"\"\n",
    "    Copy images to appropriate directories\n",
    "    \n",
    "    Args:\n",
    "        splits: Dict with train/val/test image lists\n",
    "    \"\"\"\n",
    "    for split_name, images in splits.items():\n",
    "        if split_name == 'train':\n",
    "            target_dir = train_dir\n",
    "        elif split_name == 'val':\n",
    "            target_dir = val_dir\n",
    "        else:  # test\n",
    "            target_dir = base_dir / 'test'\n",
    "            target_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        for img_path in images:\n",
    "            try:\n",
    "                shutil.copy2(img_path, target_dir / img_path.name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying {img_path}: {e}\")\n",
    "        \n",
    "        print(f\"  ‚úÖ {split_name}: {len(images)} images\")\n",
    "\n",
    "def organize_raw_dataset():\n",
    "    \"\"\"Organize raw dataset into train/val/test\"\"\"\n",
    "    print(\"\\nüîÑ Organizing dataset...\")\n",
    "    \n",
    "    valid_images, invalid = scan_dataset(raw_dir)\n",
    "    \n",
    "    if len(valid_images) == 0:\n",
    "        print(\"‚ö†Ô∏è  No valid images found in raw directory\")\n",
    "        return None\n",
    "    \n",
    "    splits = split_dataset(valid_images)\n",
    "    organize_images(splits)\n",
    "    \n",
    "    print(\"‚úÖ Dataset organization complete\")\n",
    "    return splits\n",
    "\n",
    "print(\"‚úÖ Organization functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9e2c9",
   "metadata": {},
   "source": [
    "## 3. Data Organization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c11a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_image(image_path):\n",
    "    \"\"\"\n",
    "    Validate if an image file is readable\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image file\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if valid, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(str(image_path))\n",
    "        if img is None:\n",
    "            return False\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating {image_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_image_stats(image_path):\n",
    "    \"\"\"Get image statistics\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        return None\n",
    "    \n",
    "    height, width = img.shape[:2]\n",
    "    size_kb = image_path.stat().st_size / 1024\n",
    "    \n",
    "    return {\n",
    "        'width': width,\n",
    "        'height': height,\n",
    "        'size_kb': round(size_kb, 2),\n",
    "        'channels': img.shape[2] if len(img.shape) > 2 else 1\n",
    "    }\n",
    "\n",
    "def scan_dataset(directory):\n",
    "    \"\"\"Scan and validate dataset directory\"\"\"\n",
    "    directory = Path(directory)\n",
    "    \n",
    "    valid_images = []\n",
    "    invalid_images = []\n",
    "    \n",
    "    supported_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "    \n",
    "    for file_path in directory.rglob('*'):\n",
    "        if file_path.suffix.lower() in supported_formats:\n",
    "            if validate_image(file_path):\n",
    "                valid_images.append(file_path)\n",
    "            else:\n",
    "                invalid_images.append(file_path)\n",
    "    \n",
    "    print(f\"üìä Dataset Scan Results:\")\n",
    "    print(f\"  ‚úÖ Valid images: {len(valid_images)}\")\n",
    "    print(f\"  ‚ùå Invalid images: {len(invalid_images)}\")\n",
    "    \n",
    "    return valid_images, invalid_images\n",
    "\n",
    "print(\"‚úÖ Validation functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366592f3",
   "metadata": {},
   "source": [
    "## 2. Image Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9128dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory paths\n",
    "base_dir = Path('processed_data')\n",
    "raw_dir = base_dir / 'raw'\n",
    "train_dir = base_dir / 'train'\n",
    "val_dir = base_dir / 'val'\n",
    "crops_dir = base_dir / 'crops'\n",
    "labels_file = base_dir / 'labels.json'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [raw_dir, train_dir, val_dir, crops_dir]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"‚úÖ Directory ready: {directory}\")\n",
    "\n",
    "print(\"\\nüìÅ Directory structure initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38ac3a2",
   "metadata": {},
   "source": [
    "## 1. Setup Directory Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac882de",
   "metadata": {},
   "source": [
    "# 6. EasyOCR Performance Optimization Guide\n",
    "\n",
    "## Quick Reference: When to Use Each Strategy\n",
    "\n",
    "| Image Type | Preprocessing | Threshold | Resize | Notes |\n",
    "|---|---|---|---|---|\n",
    "| Clear documents | Light | 0.6 | 1200px | Fastest, highest accuracy |\n",
    "| Normal photos | Light | 0.5 | 1200px | Balanced quality/speed |\n",
    "| Noisy/blurry | Aggressive | 0.3 | 1500px | Slower, detects more |\n",
    "| Small text | Aggressive | 0.4 | 2000-3000px | Very slow, necessary for tiny text |\n",
    "| Skewed docs | Aggressive + deskew | 0.5 | 1500px | Rotation correction first |\n",
    "| Colored background | Aggressive | 0.4 | 1500px | Focus on brightness channel |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce31547",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example: Processing Different Image Types\n",
    "\n",
    "# Load an image\n",
    "test_image_path = \"path/to/your/image.jpg\"  # Change this\n",
    "test_image = cv2.imread(test_image_path)\n",
    "\n",
    "if test_image is not None:\n",
    "    # STRATEGY 1: Clear Documents\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STRATEGY 1: Clear Documents (Fast, Accurate)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Resize moderately\n",
    "    resized = cv2.resize(test_image, (1200, int(test_image.shape[0] * 1200 / test_image.shape[1])))\n",
    "    gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Minimal denoising\n",
    "    filtered = cv2.bilateralFilter(gray, 5, 50, 50)\n",
    "    enhanced = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)).apply(filtered)\n",
    "    \n",
    "    # Use high confidence threshold\n",
    "    print(f\"‚úÖ Preprocessing: Light\")\n",
    "    print(f\"‚úÖ Resize: 1200px\")\n",
    "    print(f\"‚úÖ Confidence Threshold: 0.6\")\n",
    "    print(f\"‚úÖ Speed: Fast | Accuracy: High\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STRATEGY 2: Noisy/Blurry Images (Slower, Better Detection)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Aggressive resizing\n",
    "    resized = cv2.resize(test_image, (1500, int(test_image.shape[0] * 1500 / test_image.shape[1])))\n",
    "    gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Multi-pass denoising\n",
    "    denoised = cv2.bilateralFilter(gray, 9, 75, 75)\n",
    "    denoised = cv2.bilateralFilter(denoised, 5, 50, 50)\n",
    "    \n",
    "    # Enhance contrast\n",
    "    enhanced = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)).apply(denoised)\n",
    "    \n",
    "    # Sharpen\n",
    "    kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "    sharpened = cv2.filter2D(enhanced, -1, kernel)\n",
    "    final = cv2.addWeighted(enhanced, 0.7, sharpened, 0.3, 0)\n",
    "    \n",
    "    print(f\"‚úÖ Preprocessing: Aggressive\")\n",
    "    print(f\"‚úÖ Resize: 1500px\")\n",
    "    print(f\"‚úÖ Denoising: 2-pass bilateral filter\")\n",
    "    print(f\"‚úÖ Sharpening: Applied\")\n",
    "    print(f\"‚úÖ Confidence Threshold: 0.3\")\n",
    "    print(f\"‚úÖ Speed: Slow | Accuracy: Better\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STRATEGY 3: Small Text (Very Slow, Necessary)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Extreme resizing for small text\n",
    "    resized = cv2.resize(test_image, (2500, int(test_image.shape[0] * 2500 / test_image.shape[1])))\n",
    "    gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Aggressive preprocessing\n",
    "    denoised = cv2.bilateralFilter(gray, 9, 75, 75)\n",
    "    enhanced = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)).apply(denoised)\n",
    "    \n",
    "    print(f\"‚úÖ Resize: 2500px (Huge!)\")\n",
    "    print(f\"‚úÖ Preprocessing: Aggressive\")\n",
    "    print(f\"‚úÖ Confidence Threshold: 0.35\")\n",
    "    print(f\"‚úÖ Speed: Very Slow (~30-60s per image)\")\n",
    "    print(f\"‚ö†Ô∏è Use only when text is < 50px height\")\n",
    "else:\n",
    "    print(\"‚ùå Could not load test image. Update path above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8651d7c2",
   "metadata": {},
   "source": [
    "# 7. Implementation Checklist for Your Project\n",
    "\n",
    "## ‚úÖ What's Been Implemented in app.py\n",
    "\n",
    "### Preprocessing Functions (‚úÖ DONE)\n",
    "- `resize_for_ocr()` - Optimal resizing for different text sizes\n",
    "- `enhance_contrast()` - CLAHE contrast enhancement\n",
    "- `sharpen_image()` - Edge sharpening for blurry images\n",
    "- `deskew_image()` - Rotation correction for tilted documents\n",
    "- `preprocess_image_light()` - Fast preprocessing for high-quality images\n",
    "- `preprocess_image_aggressive()` - Enhanced preprocessing for poor-quality images\n",
    "\n",
    "### Smart Filtering (‚úÖ DONE)\n",
    "- `filter_by_confidence()` - Intelligent threshold filtering with text quality checks\n",
    "- `remove_overlapping_detections()` - Removes duplicate detections (IoU-based)\n",
    "- `improve_bbox()` - Tightens bounding boxes for better visualization\n",
    "\n",
    "### OCR Extraction (‚úÖ DONE)\n",
    "- `extract_text_with_ocr()` - Complete optimized pipeline with:\n",
    "  - ‚úÖ Confidence filtering\n",
    "  - ‚úÖ Duplicate removal\n",
    "  - ‚úÖ BBox tightening\n",
    "  - ‚úÖ Position sorting\n",
    "  - ‚úÖ Deduplication\n",
    "\n",
    "### UI Enhancements (‚úÖ DONE)\n",
    "- **Image Quality Selector** - Choose preprocessing based on image type\n",
    "- **Auto-Threshold Adjustment** - Recommended settings per quality level\n",
    "- **Debug Mode** - Shows all optimizations applied\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ How to Use the Optimizations\n",
    "\n",
    "### For Clear Documents:\n",
    "```\n",
    "1. Select: \"High (Clear Documents)\"\n",
    "2. Auto Settings: Light preprocessing, threshold=0.6\n",
    "3. Speed: ~2-3 seconds per image\n",
    "4. Accuracy: Very high\n",
    "```\n",
    "\n",
    "### For Normal Photos:\n",
    "```\n",
    "1. Select: \"Medium (Normal Photos)\"\n",
    "2. Auto Settings: Light preprocessing, threshold=0.5\n",
    "3. Speed: ~3-5 seconds per image\n",
    "4. Accuracy: High\n",
    "```\n",
    "\n",
    "### For Noisy/Blurry Images:\n",
    "```\n",
    "1. Select: \"Low (Noisy/Blurry)\"\n",
    "2. Auto Settings: Aggressive preprocessing, threshold=0.3\n",
    "3. Speed: ~10-15 seconds per image\n",
    "4. Accuracy: Good (gets more text including noise)\n",
    "```\n",
    "\n",
    "### For Small Text:\n",
    "```\n",
    "1. Select: \"Low (Noisy/Blurry)\"\n",
    "2. Manually change Preprocessing to: \"Aggressive (Enhanced Denoise)\"\n",
    "3. May need to run locally and modify resize target to 2500-3000px\n",
    "4. Speed: ~30-60 seconds per image\n",
    "5. Note: Requires code modification - see notebook section below\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß If You Want to Fine-Tune Further\n",
    "\n",
    "### To Enlarge Further for Tiny Text:\n",
    "Edit `resize_for_ocr()` target_width parameter when calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f191f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Direct Function Usage Example (For Jupyter or Scripts)\n",
    "\n",
    "# If you want to use the optimizations directly without Streamlit:\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import easyocr\n",
    "\n",
    "# Load reader\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(\"your_image.jpg\")\n",
    "\n",
    "# ---- EXAMPLE 1: Process High-Quality Document ----\n",
    "print(\"Processing high-quality document...\")\n",
    "\n",
    "# Step 1: Light preprocessing\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "filtered = cv2.bilateralFilter(gray, 5, 50, 50)\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "enhanced = clahe.apply(filtered)\n",
    "\n",
    "# Step 2: Extract with high confidence\n",
    "results = reader.readtext(enhanced)\n",
    "high_conf_results = [(bbox, text, conf) for bbox, text, conf in results if conf >= 0.6]\n",
    "\n",
    "print(f\"Found {len(high_conf_results)} high-confidence detections\")\n",
    "\n",
    "# ---- EXAMPLE 2: Process Noisy Image ----\n",
    "print(\"\\nProcessing noisy image...\")\n",
    "\n",
    "# Step 1: Aggressive preprocessing\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Multi-pass denoising\n",
    "denoised = cv2.bilateralFilter(gray, 9, 75, 75)\n",
    "denoised = cv2.bilateralFilter(denoised, 5, 50, 50)\n",
    "\n",
    "# Enhance and sharpen\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "enhanced = clahe.apply(denoised)\n",
    "\n",
    "kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "sharpened = cv2.filter2D(enhanced, -1, kernel)\n",
    "final = cv2.addWeighted(enhanced, 0.7, sharpened, 0.3, 0)\n",
    "\n",
    "# Step 2: Extract with lower confidence\n",
    "results = reader.readtext(final)\n",
    "filtered_results = [(bbox, text, conf) for bbox, text, conf in results if conf >= 0.3]\n",
    "\n",
    "print(f\"Found {len(filtered_results)} detections (confidence >= 0.3)\")\n",
    "\n",
    "# ---- EXAMPLE 3: Remove Duplicates ----\n",
    "print(\"\\nRemoving duplicate detections...\")\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate Intersection over Union\"\"\"\n",
    "    x1_coords = [p[0] for p in box1]\n",
    "    x1_min, x1_max = min(x1_coords), max(x1_coords)\n",
    "    y1_coords = [p[1] for p in box1]\n",
    "    y1_min, y1_max = min(y1_coords), max(y1_coords)\n",
    "    \n",
    "    x2_coords = [p[0] for p in box2]\n",
    "    x2_min, x2_max = min(x2_coords), max(x2_coords)\n",
    "    y2_coords = [p[1] for p in box2]\n",
    "    y2_min, y2_max = min(y2_coords), max(y2_coords)\n",
    "    \n",
    "    x_inter = max(0, min(x1_max, x2_max) - max(x1_min, x2_min))\n",
    "    y_inter = max(0, min(y1_max, y2_max) - max(y1_min, y2_min))\n",
    "    inter_area = x_inter * y_inter\n",
    "    \n",
    "    box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    if union_area == 0:\n",
    "        return 0\n",
    "    return inter_area / union_area\n",
    "\n",
    "# Remove overlapping detections\n",
    "unique_results = []\n",
    "for bbox1, text1, conf1 in filtered_results:\n",
    "    is_duplicate = False\n",
    "    \n",
    "    for bbox2, text2, conf2 in unique_results:\n",
    "        if calculate_iou(bbox1, bbox2) > 0.3:\n",
    "            # Overlaps significantly\n",
    "            if conf1 > conf2:\n",
    "                unique_results.remove((bbox2, text2, conf2))\n",
    "            else:\n",
    "                is_duplicate = True\n",
    "                break\n",
    "    \n",
    "    if not is_duplicate:\n",
    "        unique_results.append((bbox1, text1, conf1))\n",
    "\n",
    "print(f\"After deduplication: {len(unique_results)} unique detections\")\n",
    "print(\"\\nDone! Use unique_results for visualization or export.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
